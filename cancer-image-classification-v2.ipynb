{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":30700,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cancer Detection and Data background\n\nThe project involves building CNN classifiers from scratch to detect cancer cells in images. The competition focuses on finding a single cancerous cell in a 32x32 square at the center of each image, making it very challenging. Since there is no competition here (it is over), the classifiers will label the entire image as malignant or benign. This simplifies the training process.\n\nThe notebook will explore a few CNN iterations. Different activation functions, layers, and hyperparameters will be tested.\n\nThe curated Kaggle dataset will be used, available [here](https://www.kaggle.com/competitions/histopathologic-cancer-detection/data). This dataset, provided by Bas Veeling and others, has removed duplicate images and is hosted by Kaggle for practice. The original dataset can be found on [GitHub](https://github.com/basveeling/pcam) under the CC0 License. \n","metadata":{}},{"cell_type":"code","source":"# for stability, set the version of tensorflow\n#!pip install wurlitzer\n#!pip install tensorflow==2.13.0\n#!pip install tensorflow[and-cuda]\n#!pip install tensorflow-io","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard Kaggle info\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf \n# the below is needed to fix cuDNN registration issues, put immediately after the tf import\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nimport tensorflow_io as tfio\n\nimport numpy as np \nimport pandas as pd \nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import AvgPool2D,BatchNormalization, Conv2D, Dense, Flatten, Input, GlobalAveragePooling2D, Dropout \nfrom keras.layers import MaxPool2D, MaxPooling2D, ReLU, concatenate\nimport math, gc, copy\n\nAUTOTUNE = tf.data.AUTOTUNE\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\n\nprint(\"Tensorflow Version In Use: \", tf.__version__ )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we detect our hardware and light up GPUs or TPUs if we have them.","metadata":{}},{"cell_type":"code","source":"# Detect hardware and GPUs/TPUs (useful code here is copied from code search, not original)\ntry:\n     # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n    # instantiate a distribution strategy\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n \n    # report results\n    print('Running on TPU ', tpu.cluster_spec().as_dict())\n\nexcept ValueError: # If TPU not found\n    tpu = None\n    tpu_strategy = tf.distribute.get_strategy() # Default strategy that works on CPU and single GPU\n    print('Running on CPU instead')\n\nprint(\"Number of accelerators: \", tpu_strategy.num_replicas_in_sync)\nprint(\"TPU: \", tpu)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T20:28:27.012239Z","iopub.status.idle":"2024-05-27T20:28:27.012567Z","shell.execute_reply.started":"2024-05-27T20:28:27.012399Z","shell.execute_reply":"2024-05-27T20:28:27.012416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\ntry:\n    if gpus:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n\nexcept RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T20:28:30.850125Z","iopub.execute_input":"2024-05-27T20:28:30.850628Z","iopub.status.idle":"2024-05-27T20:28:30.899467Z","shell.execute_reply.started":"2024-05-27T20:28:30.850575Z","shell.execute_reply":"2024-05-27T20:28:30.898236Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpus \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gpus:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# Currently, memory growth needs to be the same across GPUs\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"],"ename":"NameError","evalue":"name 'tf' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Load Data\n\nLoad data and show basic information to describe the data files.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')\n\n# get basic info\nprint(df.info())\nprint('')\nprint(df.describe())\nprint('')\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of the 220 thousand rows of data, 40% of the observations have a positive cancer label (mean of the binary 1 over the count). The labels iare binary (1,0) with no null values in either column.\n\nAnalysis of the counts by the classification lables show an imbalance in the data.  Most are negative (no cancer/benign).  ","metadata":{}},{"cell_type":"code","source":"# Count the occurrences of each category\ncategory_counts = df['label'].value_counts()\n\n# Plot the bar chart\nplt.figure(figsize=(8, 6))\ncategory_counts.plot(kind='bar', color=['blue', 'orange'])\nplt.title('Cancer classifications')\nplt.xlabel('Positive/Negative detection')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imagelist = os.listdir('/kaggle/input/histopathologic-cancer-detection/train')\nprint(\"Image List length:\", len(imagelist))\n\nprint (\"Image list type:\", type(imagelist))\nprint (\"First record type: \", type(imagelist[0]))\nprint (\"First image values: \", imagelist[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of rows match the number of images in the training folder. Raw data is shown, the file contains the image file name.","metadata":{}},{"cell_type":"code","source":"# Load sample submission data\n\ndfss = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/sample_submission.csv')\n# describe data\nprint(dfss.describe())\nprint('')\nprint(dfss.info())\nprint('')\n\nimagesslist = os.listdir('/kaggle/input/histopathologic-cancer-detection/test/')\nprint(\"Validation Image List length:\", len(imagesslist))\n\n# raw data\ndfss.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The validation set has no labels, all are 0.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(3,4) \ncol = 0\nrow = 0\nfor i in range(12):\n    imid = df.id.sample(1).values[0]\n    image = Image.open('/kaggle/input/histopathologic-cancer-detection/train/'+imid+'.tif')\n    axs[col,row].imshow(image)\n    if col == 2:\n        row +=1\n        col = 0\n    else:\n        col +=1\nplt.show();\nprint(\"Image Specifications: Shape\",image.size,\"\\nFormat:\",image.info )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data handling in preperation for modeling\nPreprocess data to better balance the data set","metadata":{}},{"cell_type":"code","source":"randomseed = 9\n\n# rebalance so classes of positive and negative are same size, this will help in improving classification training\ntrainsize = int(df.label.sum() * .7)\ntestsize = df.label.sum()-trainsize\ncancer = pd.DataFrame(df.index[df['label'] ==1].tolist(),columns=['id'])\nnegative = pd.DataFrame(df.index[df['label'] ==0].tolist(),columns=['id'])\n\n# random sample the split\ncancer_train = cancer['id'].sample(trainsize,replace=False,random_state = randomseed)\nnegative_train = negative['id'].sample(trainsize,replace=False,random_state = randomseed)\n\nprint(\"Cancer train size:\",len(cancer_train))\nprint(\"Negative train size:\",len(negative_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mark which rows to use for train and for test\ncancer['train'] = 0\nnegative['train'] = 0\ncancer['test'] = 0\nnegative['test'] = 0\n\nfor i in range(len(cancer_train)):\n    cancer['train'].loc[cancer['id'] == cancer_train.iat[i]] = 1\n    negative['train'].loc[negative['id'] == negative_train.iat[i]] = 1\n\n# test set samplinig\ncancer_test = cancer['id'].loc[cancer['train']==0].sample(testsize,\\\n                                    replace=False,random_state=randomseed)\nnegative_test = negative['id'].loc[negative['train']==0].sample(testsize,\\\n                                    replace=False,random_state=randomseed)\n\nfor i in range(len(cancer_test)):\n    cancer['test'].loc[cancer['id'] == cancer_test.iat[i]] ==1\n    negative['test'].loc[negative['id'] == negative_test.iat[i]] ==1\n\n# compare the test size counts\nprint(\"Cancer test size:\",len(cancer_test))\nprint(\"Negative test size:\",len(negative_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The index numbers were used to randomly select train/test sets with balanced classes.\nThe dataframes for the train and test set will be built, complete with image paths.","metadata":{}},{"cell_type":"code","source":"def image_path(id_filename):\n    return f\"/kaggle/input/histopathologic-cancer-detection/train/{id_filename}.tif\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cancer_train_df = df.loc[df.index[cancer_train.tolist()]]\nnegative_train_df = df.loc[df.index[negative_train.tolist()]]\n\ntrain_df = pd.concat([cancer_train_df, negative_train_df])\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntrain_df = train_df.sample(frac=1, random_state=randomseed).reset_index(drop=True)\n\ntrain_df['path'] = train_df.id.apply(image_path)\nprint(\"Combined train length:\",len(train_df.id),\" Cancer positives:\",train_df.label.sum())\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do same for test data\ncancer_test_df = df.loc[df.index[cancer_test.tolist()]]\nnegative_test_df = df.loc[df.index[negative_test.tolist()]]\n\ntest_df = pd.concat([cancer_test_df, negative_test_df])\ntest_df = test_df.sample(frac=1).reset_index(drop=True)\ntest_df = test_df.sample(frac=1, random_state=randomseed).reset_index(drop=True)\n\ntest_df['path'] = test_df.id.apply(image_path)\nprint(\"Combined test length:\",len(test_df.id),\" Cancer positives:\",test_df.label.sum())\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Turn dataframes into numpy arrays for the cnn training","metadata":{}},{"cell_type":"code","source":"# define tr function to apply later - NOTE: this code is adapted from another notebook, very useful\n\n@tf.function\ndef grab_images(path):\n    file = tf.io.read_file(path)\n    img = tfio.experimental.image.decode_tiff(file, index=0)\n    img = tf.image.random_flip_left_right(img, seed=None)\n    img = tf.image.random_flip_up_down(img, seed=None)\n    img =img[:,:,0:-1]\n    img = img/255\n    img = tf.image.convert_image_dtype(img,dtype=tf.float32)\n    return img\n\n# test it\ntest_image = grab_images('/kaggle/input/histopathologic-cancer-detection/train/ff1dd7be24e74d29d5a91862179703eadfe8fe43.tif')\nplt.imshow(test_image)\nplt.show()\n# check normalized between 0-1\nprint(test_image[0:5,0:5,:])\n\n# check for right shape for RGBA (4th channel is pixel intensity of 1)\ntest_image.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get training datasets together\ntrain_labels = tf.data.Dataset.from_tensor_slices(np.array([np.array([0,1]) if i ==1 else np.array([1,0]) for i in train_df.label.values]))\ntrain_paths = tf.data.Dataset.from_tensor_slices(np.array([path for path in train_df.path.values]))\ntrain_imgs = train_paths.map(grab_images)\ntrain_set = tf.data.Dataset.zip((train_imgs,train_labels)).batch(64).prefetch(AUTOTUNE)\n\n# get test dataset together\ntest_labels = tf.data.Dataset.from_tensor_slices(np.array([np.array([0,1]) if i ==1 else np.array([1,0]) for i in test_df.label.values]))\ntest_paths = tf.data.Dataset.from_tensor_slices(np.array([path for path in test_df.path.values]))\ntest_imgs = test_paths.map(grab_images)\ntest_set = tf.data.Dataset.zip((test_imgs,test_labels)).batch(64).prefetch(AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_filepath =''\n#define the callbacks for upcoming models\n# earlyst = tf.keras.callbacks.EarlyStopping(monitor=\"binary_crossentropy\", \n#                                            patience = 5)\nearlyst = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n                                           patience = 5)\n\n# rlrop = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"binary_crossentropy\", \n#                                              factor=.1,\n#                                              patience = 2,\n#                                              min_lr = 0)\n\nrlrop = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", \n                                             factor=.1,\n                                             patience = 2,\n                                             min_lr = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN models\n\nMultiple attempts at CNN, an example model is shown.\n\n\n## Starting CNN\n5 layers of a convolution and an average pooling layer, followed by 3 dense layers of neurons with final activation of tanh and a Binary Cross Entropy Loss Function to choose between the two classes. ","metadata":{}},{"cell_type":"code","source":" with tpu_strategy.scope():\n    model = Sequential([\n    Input(shape=(96, 96, 3)),  \n   \n    Conv2D(32, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),      \n    \n    Conv2D(32, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n \n    Conv2D(64, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n    \n    Conv2D(64, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n \n    Conv2D(32, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n \n    # build the ANN layers\n    Flatten(),\n    Dense(288, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(2, activation='tanh')\n    ])\n    \n    model.compile(\n     optimizer =    tf.keras.optimizers.RMSprop(\n            learning_rate=0.0005,\n            momentum=0.18,\n        ),\n        loss= keras.losses.BinaryCrossentropy(from_logits=True), # for tf v 2.15\n        # loss= 'BinaryCrossentropy', # for tf v 2.13\n        metrics=[ 'BinaryCrossentropy', 'accuracy']\n    )\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define model save location\ncheckpoint_filepath = '/kaggle/working/model_basic_cnn/'\n!mkdir {checkpoint_filepath}\ncheckpoint_filename = 'checkpoint.model.keras'\ncheckpoint_fullpath = checkpoint_filepath + '' + checkpoint_filename\n# checkpoint_fullpath = checkpoint_filepath # for tf v 2.13\n                                           \n# checkpoints = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n#     filepath=checkpoint_fullpath,\n#     save_weights_only=False,\n#     monitor='val_accuracy',\n#     mode='max',\n#     save_best_only=True)\n\ncheckpoints = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_fullpath,\n    save_weights_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set tf functions to run eagerly\ntf.config.run_functions_eagerly(True)\n\n# fit model\nhistory = model.fit(\n                    train_set,\n                    epochs=10, #ran before at 20, set to 10 to keep reasonable on lenght of time to run\n                    callbacks=[rlrop,earlyst,checkpoints],\n                    validation_data = test_set\n                    )\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for cross entropy loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nprint(\"Make a few predictions\")\nx = model.predict(test_set.take(1)) #batch size is 64\n\nprint(\"Binary decision logits (first 10):\\n\",x[0:10])\n\nprint(\"Predictions:\\n\",[np.argmax(x) for x in x[0:30]],'\\nGround Truth:\\n',[x for x in test_df.label.values[0:30]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis of Results from Initial Model\nAnalysing the training versus test loss over training epochs shows it is likely optimal around XX epochs.","metadata":{}},{"cell_type":"markdown","source":"## Variation of Architecture\nAdd a layer of convolution to determine impact.  This may allow the model to learning a higher order of features that improve classifications","metadata":{}},{"cell_type":"code","source":" with tpu_strategy.scope():\n    model2 = Sequential([\n    Input(shape=(96, 96, 3)),  \n   \n    Conv2D(32, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),      \n    \n    Conv2D(32, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n \n    Conv2D(64, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n    \n    Conv2D(64, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n \n    Conv2D(32, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n    \n    # Additional Conv/pooling layer\n    Conv2D(32, 3, padding='same', activation = 'relu'),\n    AvgPool2D(pool_size=2, padding='same'),\n \n    # build the ANN layers\n    Flatten(),\n    Dense(288, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(2, activation='tanh')\n    ])\n    \n    model2.compile(\n     optimizer =    tf.keras.optimizers.RMSprop(\n            learning_rate=0.0005,\n            momentum=0.18,\n        ),\n        loss= keras.losses.BinaryCrossentropy(from_logits=True), # for tf v 2.15\n        # loss= 'BinaryCrossentropy', # for tf v 2.13\n        metrics=[ 'BinaryCrossentropy', 'accuracy']\n    )\n\nmodel2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define model save location\ncheckpoint_filepath = '/kaggle/working/model_xtralayer_cnn/'\n!mkdir {checkpoint_filepath}\ncheckpoint_filename = 'checkpoint.model.keras'\ncheckpoint_fullpath = checkpoint_filepath + '' + checkpoint_filename\n# checkpoint_fullpath = checkpoint_filepath # for tf v 2.13\n                                           \n# checkpoints = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n#     filepath=checkpoint_fullpath,\n#     save_weights_only=False,\n#     monitor='val_accuracy',\n#     mode='max',\n#     save_best_only=True)\n\ncheckpoints = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_fullpath,\n    save_weights_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set tf functions to run eagerly\ntf.config.run_functions_eagerly(True)\n\n# fit model\nhistory2 = model2.fit(\n                    train_set,\n                    epochs=10,  # keep to 10 epochs to save time\n                    callbacks=[rlrop,earlyst,checkpoints],\n                    validation_data = test_set\n                    )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history2.history['accuracy'])\nplt.plot(history2.history['val_accuracy'])\nplt.title('model v2 accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nprint(\"Make a few predictions with v2\")\nx = model2.predict(test_set.take(1)) #batch of 64\n\nprint(\"Binary decision Logits v2 (firt 10):\\n\",x[0:10])\n\nprint(\"Predictions v2:\\n\",[np.argmax(x) for x in x[0:30]],'\\nGround Truth:\\n',[x for x in test_df.label.values[0:30]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"from kerastuner.tuners import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\n\ntuner = RandomSearch(\n    model2,\n    objective='val_accuracy',\n    max_trials=3,  \n    directory='cancerclassifier_tuner_dir',  \n    project_name='cancerclassifier_v2'\n)\n\ntuner.search(train_set, epochs=5, validation_data=test_set, validation_steps=VA_STEPS)\n\n# Find the best hyperparameters\nbest_hypers = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(\"Best hyperparameters:\\n\", best_hypers)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Save model\n","metadata":{}},{"cell_type":"code","source":"# Save the model\nfinal_model.save('Cancer_Image_Model.h5')\n\n# Save the history\nwith open('Cancer_Image_Model_history.pkl', 'wb') as file:\n    pickle.dump(history.history, file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions\nCNN results vary, etc.","metadata":{}},{"cell_type":"markdown","source":"#### Attributions\nSource references:\nHCD_SB6: https://www.kaggle.com/code/shayjohnson/hcd-sb-6\n\nKaggle environment setup code: https://keras.io/getting_started/\n\nBinary Classification: https://www.kaggle.com/code/toddgardiner/binary-cancer-classifier-s-tf-tpu\n\nGPU usage code: https://github.com/tensorflow/tensorflow/issues/64177\n\nCuDNN and similar code: https://github.com/tensorflow/tensorflow/issues/62075\n\nTensflow runtime graph vs eagerly: https://www.tensorflow.org/guide/intro_to_graphs\n\nTensorflow versioning issues and CUDA on Kaggle: https://github.com/tensorflow/tensorflow/issues/64177","metadata":{}}]}